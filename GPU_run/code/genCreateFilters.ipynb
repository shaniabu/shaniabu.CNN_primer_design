{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171fd400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set (840,)\n",
      "vectorSize 30818\n",
      "714\n",
      "27144\n",
      "(714,)\n",
      "42\n",
      "(42,)\n",
      "84\n",
      "(84,)\n",
      "8\n",
      "714\n",
      "8\n",
      "(714, 8)\n",
      "42\n",
      "8\n",
      "(42, 8)\n",
      "84\n",
      "8\n",
      "(84, 8)\n",
      "data set (840,)\n",
      "vectorSize 30818\n",
      "714\n",
      "27144\n",
      "(714,)\n",
      "42\n",
      "(42,)\n",
      "84\n",
      "(84,)\n",
      "8\n",
      "714\n",
      "8\n",
      "(714, 8)\n",
      "42\n",
      "8\n",
      "(42, 8)\n",
      "84\n",
      "8\n",
      "(84, 8)\n",
      "17\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "(?, 1, 30818, 1)\n",
      "(?, 1, 209, 12)\n",
      "(?, 8)\n",
      "(?, 8)\n",
      "pad_left Conv 7.0\n",
      "pad_right Conv 7.0\n",
      "out_width HPool 209.0\n",
      "pad_left HPool 57.0\n",
      "pad_right HPool 57.0\n",
      "data set (840,)\n",
      "vectorSize 30818\n",
      "8\n",
      "840\n",
      "8\n",
      "(840, 8)\n",
      "(1, 15, 1, 12)\n",
      "(840, 1, 30818, 12)\n",
      "(840, 1, 209, 12)\n"
     ]
    }
   ],
   "source": [
    "#Declarations******************************************************************************\n",
    "#from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from funcCNN import *\n",
    "from crossValB import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "dataFolder='../data/'\n",
    "\n",
    "\n",
    "#Parameters*******************************************************************************\n",
    "#Maximum number of iterations\n",
    "#iterMax=int(sys.argv[1])\n",
    "#maximum number of iterations\n",
    "limit=1.00\n",
    "#regularization on the weights\n",
    "beta=0.001\n",
    "#version of the code\n",
    "version='gen1'\n",
    "#size of batch\n",
    "batchSize=42\n",
    "#Parameters*******************************************************************************\n",
    "#w1=int(sys.argv[2]) #12\n",
    "#w4=int(sys.argv[3]) #196\n",
    "#h1=int(sys.argv[4]) #148\n",
    "#wd1=int(sys.argv[5]) #21\n",
    "#index=int(sys.argv[6]) #0\n",
    "#kfoldIndex=int(sys.argv[7]) #0\n",
    "#generation=int(sys.argv[8]) #0\n",
    "\n",
    "w1=12\n",
    "w4=196\n",
    "h1=148\n",
    "wd1=15\n",
    "index=0\n",
    "kfoldIndex=0\n",
    "generation=0\n",
    "\n",
    "\n",
    "#Input Data***********************************************************************************\n",
    "(test_dataset_Flat,oneHot_test_labels,valid_dataset_Flat,oneHot_valid_labels,\n",
    "\ttrain_dataset_Flat,oneHot_train_labels,labelSize,vectorSize)=get_Info(\n",
    "\tkfoldIndex,dataFolder) #from crossValB\n",
    "runs=int(len(oneHot_train_labels)/batchSize)\n",
    "print(runs)\n",
    "#Model declaration************************************************************************\n",
    "import tensorflow as tf\n",
    "#declare interactive session\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "sess=tf.InteractiveSession()\n",
    "#INPUT->CONV LAYER->CONV LAYER->CONV LAYER->RECT FLAT->RECT DROPOUT\n",
    "\n",
    "#function to declare easily the weights only by shape\n",
    "def weight_variable(shape):\n",
    "\tinitial = tf.truncated_normal(shape, stddev=0.1)\n",
    "\treturn tf.Variable(initial)\n",
    "#function to declare easily the bias only by shape\n",
    "def bias_variable(shape):\n",
    "\tinitial = tf.constant(0.1, shape=shape)\n",
    "\treturn tf.Variable(initial)\n",
    "\n",
    "#input variable\n",
    "x = tf.placeholder(tf.float32, [None, vectorSize])\n",
    "#keep probability to change from dropout 0.50 to 1.0 in validation and test\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "#expected outputs variable\n",
    "y_ = tf.placeholder(tf.float32, [None, labelSize])\n",
    "\n",
    "#arrange the tensor as an image (1*31029) 1 channel\n",
    "x_image0 = tf.reshape(x, [-1,1,vectorSize,1])\n",
    "x_image = tf.transpose(x_image0, perm=[0,3,2,1])\n",
    "#arrange the tensor into 1 channels (1*31029)\n",
    "\n",
    "#1 LAYER*************************************************************************************\n",
    "#1 Convolutional Layer Explicit for regularization of the weights\n",
    "#weigth first layer 1 input channels, 12 output channels, 1x21 filter window size\n",
    "W_conv1 = weight_variable([1, wd1, 1, w1])\n",
    "#bias declaration the size has to be the same as the output channels 12\n",
    "b_conv1 = bias_variable([w1])\n",
    "#convolution (input weights) moving 1 step each time with a relu\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, \n",
    "\tstrides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "#max pooling with a 148 width window size, moving 148 in width by step\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 1, h1, 1],\n",
    "\tstrides=[1, 1, h1, 1], padding='SAME')\n",
    "#output=545/4\n",
    "#1 LAYER*************************************************************************************\n",
    "\n",
    "#Rectifier LAYER*****************************************************************************\n",
    "#calculated coefficient for the flattening from the size of the 3 convolutional layer\n",
    "coef=int (h_pool1.get_shape()[1]*h_pool1.get_shape()[2]*h_pool1.get_shape()[3])\n",
    "h_pool2_flat = tf.reshape(h_pool1, [-1, coef])\n",
    "#declare the weights considering the constants and 256 output \n",
    "W_fc1 = weight_variable([coef, w4])\n",
    "b_fc1 = bias_variable([w4])\n",
    "\n",
    "#rectifier (matmul)\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "#Rectifier LAYER*****************************************************************************\n",
    "\n",
    "#Rectifier-Dropout LAYER**********************************************************************\n",
    "#dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "#declare weights with the ouput layer in this case 2 (labelSize)\n",
    "W_fc2 = weight_variable([w4, labelSize])\n",
    "b_fc2 = bias_variable([labelSize])\n",
    "#output\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "#Rectifier-Dropout LAYER**********************************************************************\n",
    "\n",
    "#Loss Function********************************************************************************\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[0]))\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_conv, labels=y_)+\n",
    "\tbeta*tf.nn.l2_loss(W_conv1))\n",
    "#Optimizer Adam at 1e-5 (literature)**********************************************************\n",
    "train_step = tf.train.AdamOptimizer(1e-5).minimize(cross_entropy)\n",
    "#softmax prediction remember we are using one hot labels\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "\n",
    "trueResult=tf.argmax(y_conv,1)\n",
    "trueTest=tf.argmax(y_,1)\n",
    "#accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#Loss Function********************************************************************************\n",
    "valid_accuracy_global=0.0\n",
    "test_accuracy_global=0.0\n",
    "\n",
    "\n",
    "#start\n",
    "sess.run(tf.initialize_all_variables())\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"model/model.ckpt\")\n",
    "#saver.restore(sess, \"..data/model/model.ckpt\")\n",
    "#Extra to verify sizes************************************************************************\n",
    "print(x_image.get_shape())\n",
    "#print(h_conv1.get_shape())\n",
    "print(h_pool1.get_shape())\n",
    "\n",
    "print(y_conv.get_shape())\n",
    "print(y_.get_shape())\n",
    "#Extra to verify sizes************************************************************************\n",
    "\n",
    "\n",
    "#out_height = ceil(float(in_height) / float(strides[1]))\n",
    "\n",
    "#strides2=strides[2]\n",
    "strides2=1\n",
    "in_width=vectorSize\n",
    "filter_width=15\n",
    "out_width  = np.ceil(float(in_width) / float(strides2))\n",
    "#pad_along_height = max((out_height - 1) * strides[1] +filter_height - in_height, 0)\n",
    "pad_along_width = max((out_width - 1) * strides2 +filter_width - in_width, 0)\n",
    "#pad_top = pad_along_height // 2\n",
    "#pad_bottom = pad_along_height - pad_top\n",
    "pad_left = pad_along_width // 2\n",
    "pad_right = pad_along_width - pad_left\n",
    "\n",
    "print(\"pad_left Conv \"+str(pad_left))\n",
    "print(\"pad_right Conv \"+str(pad_right))\n",
    "\n",
    "strides2=148\n",
    "in_width=vectorSize\n",
    "filter_width=148\n",
    "out_width  = np.ceil(float(in_width) / float(strides2))\n",
    "#pad_along_height = max((out_height - 1) * strides[1] +filter_height - in_height, 0)\n",
    "pad_along_width = max((out_width - 1) * strides2 +filter_width - in_width, 0)\n",
    "#pad_top = pad_along_height // 2\n",
    "#pad_bottom = pad_along_height - pad_top\n",
    "pad_left = pad_along_width // 2\n",
    "pad_right = pad_along_width - pad_left\n",
    "\n",
    "print(\"out_width HPool \"+str(out_width))\n",
    "print(\"pad_left HPool \"+str(pad_left))\n",
    "print(\"pad_right HPool \"+str(pad_right))\n",
    "\t\t\t\t  \n",
    "#Input Data***********************************************************************************\n",
    "(data,oneHotLabels)=get_InfoTotal('../data/')\n",
    "\n",
    "#Output:ay amo\n",
    "units = sess.run(W_conv1,feed_dict={x:data, \n",
    "\t\t\ty_: oneHotLabels, keep_prob: 1.0})\n",
    "print(units.shape)\n",
    "units = sess.run(h_conv1,feed_dict={x:data, \n",
    "\t\t\ty_: oneHotLabels, keep_prob: 1.0})\n",
    "print(units.shape)\n",
    "\n",
    "dataSize=data.shape\n",
    "sampleSize=int(dataSize[0])\n",
    "\n",
    "Mat=np.zeros((sampleSize, vectorSize))\n",
    "import pandas as pd\n",
    "for filterIndex in range(0,units.shape[3]):\n",
    "\tfor testSize in range(0,sampleSize):\n",
    "\t\tfor inputSize in range (0,vectorSize):\n",
    "\t\t\tMat[testSize][inputSize]=units[testSize][0][inputSize][filterIndex]\n",
    "\tpd.DataFrame(Mat).to_csv(\"../data/filters/filter_\"+str(filterIndex)+\".csv\", header=None, index =None)\n",
    "\n",
    "\n",
    "units = sess.run(h_pool1,feed_dict={x:data, \n",
    "\t\t\ty_: oneHotLabels, keep_prob: 1.0})\n",
    "print(units.shape)\n",
    "Mat=np.zeros((sampleSize, units.shape[2]))\n",
    "import pandas as pd\n",
    "for filterIndex in range(0,units.shape[3]):\n",
    "\tfor testSize in range(0,sampleSize):\n",
    "\t\tfor inputSize in range (0,units.shape[2]):\n",
    "\t\t\tMat[testSize][inputSize]=units[testSize][0][inputSize][filterIndex]\n",
    "\tpd.DataFrame(Mat).to_csv(\"../data/filters/maxPool_\"+str(filterIndex)+\".csv\", header=None, index =None)\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b2770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
